{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c9b739d7-5903-4368-867e-16b9d7136692",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/project-ida/arpa-e-experiments/blob/main/templates/eds-spectrum-starter.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href=\"https://nbviewer.org/github/project-ida/arpa-e-experiments/blob/main/templates/eds-spectrum-starter.ipynb\" target=\"_parent\"><img src=\"https://nbviewer.org/static/img/nav_logo.svg\" alt=\"Open In nbviewer\" width=\"100\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f188c81",
   "metadata": {
    "id": "7f188c81"
   },
   "source": [
    "# EDS Spectrum template\n",
    "\n",
    "This notebook provides a quick, end-to-end demo for ROI-based EDS analysis:\n",
    "it pulls a saved ROI set from the Surface Viewer API, builds a table of selected tiles,\n",
    "loads the corresponding spectra, aggregates them, and runs a simple first-pass peak analysis.\n",
    "\n",
    "## How to use\n",
    "\n",
    "1. Add the `api_url` (get the link from a surface viewer selection grid)\n",
    "name.\n",
    "2. Run cells top-to-bottom.\n",
    "3. Use the aggregate plot and peak table as a rapid “first look” before moving to the full testbed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Z5okfpEzLR6J",
   "metadata": {
    "id": "Z5okfpEzLR6J"
   },
   "outputs": [],
   "source": [
    "api_url = 'CHANGE_ME'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b17d9ed",
   "metadata": {
    "id": "1b17d9ed"
   },
   "outputs": [],
   "source": [
    "\n",
    "import requests\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "from urllib.parse import urlparse, parse_qs, urljoin, quote\n",
    "\n",
    "import re\n",
    "import requests\n",
    "\n",
    "from scipy.signal import savgol_filter, find_peaks, peak_widths\n",
    "from scipy import sparse\n",
    "from scipy.sparse.linalg import spsolve\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# ROI loading from the API JSON\n",
    "# -----------------------------\n",
    "\n",
    "def load_roi_api(api_url: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Fetch ROI selections from:\n",
    "        .../api/rois.php?dataset=<dataset>&name=<roi_name>\n",
    "\n",
    "    Expected response schema:\n",
    "        { \"selections\": [{col,row,srcJson,basename,foldername}, ...] }\n",
    "\n",
    "    Returns a DataFrame with at least:\n",
    "        col, row, srcJson, basename, foldername\n",
    "    \"\"\"\n",
    "    r = requests.get(api_url, timeout=60)\n",
    "    r.raise_for_status()\n",
    "    data = r.json()\n",
    "    selections = data.get(\"selections\", [])\n",
    "    df = pd.DataFrame(selections)\n",
    "    if df.empty:\n",
    "        return df\n",
    "\n",
    "    # types\n",
    "    for c in [\"row\", \"col\"]:\n",
    "        if c in df.columns:\n",
    "            df[c] = df[c].astype(\"int64\")\n",
    "\n",
    "    # normalise expected string columns if present\n",
    "    for c in [\"srcJson\", \"basename\", \"foldername\"]:\n",
    "        if c in df.columns:\n",
    "            df[c] = df[c].astype(\"string\")\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def infer_dataset_base_from_api(api_url: str) -> str:\n",
    "    \"\"\"\n",
    "    Try to infer the dataset base URL for spectrum JSON files based on the\n",
    "    'dataset' query parameter used by the ROI API.\n",
    "    \"\"\"\n",
    "    p = urlparse(api_url)\n",
    "    qs = parse_qs(p.query)\n",
    "    dataset = (qs.get(\"dataset\") or [None])[0]\n",
    "    if not dataset:\n",
    "        raise ValueError(\"Could not find 'dataset' parameter in the API URL.\")\n",
    "\n",
    "    # Root pattern used by Surface Viewer\n",
    "    root = f\"{p.scheme}://{p.netloc}/surface-viewer/data/\"\n",
    "\n",
    "    # URL-encode the dataset name to form a safe folder path\n",
    "    dataset_encoded = quote(dataset, safe=\"\")\n",
    "    return urljoin(root, dataset_encoded + \"/\")\n",
    "\n",
    "\n",
    "def add_json_urls(df: pd.DataFrame, api_url: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Add a json_url column by resolving srcJson against the inferred dataset base.\n",
    "    \"\"\"\n",
    "    if df.empty:\n",
    "        df[\"json_url\"] = pd.Series(dtype=\"string\")\n",
    "        return df\n",
    "\n",
    "    dataset_base = infer_dataset_base_from_api(api_url)\n",
    "    df = df.copy()\n",
    "    df[\"json_url\"] = df[\"srcJson\"].apply(lambda p: urljoin(dataset_base, str(p)))\n",
    "    return df\n",
    "\n",
    "\n",
    "# -----------------------------------------\n",
    "# Minimal aggregation helpers (from testbed)\n",
    "# -----------------------------------------\n",
    "\n",
    "def _new_session() -> requests.Session:\n",
    "    s = requests.Session()\n",
    "    s.headers.update({\"User-Agent\": \"eds-demo-notebook/0.1\"})\n",
    "    return s\n",
    "\n",
    "\n",
    "def fetch_json_items(url: str, session: requests.Session) -> list:\n",
    "    \"\"\"\n",
    "    GET a JSON file and return a list of records.\n",
    "    Supports files that are either:\n",
    "        - an array of records, or\n",
    "        - { \"items\": [ ... ] }.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        r = session.get(url, timeout=60)\n",
    "        r.raise_for_status()\n",
    "        data = r.json()\n",
    "        if isinstance(data, dict) and isinstance(data.get(\"items\"), list):\n",
    "            return data[\"items\"]\n",
    "        if isinstance(data, list):\n",
    "            return data\n",
    "        return []\n",
    "    except Exception:\n",
    "        return []\n",
    "\n",
    "\n",
    "def build_spectrum_index(urls: list[str], *, progress: bool = True,\n",
    "                         session: requests.Session | None = None) -> dict[tuple[str,int,int], list[int]]:\n",
    "    \"\"\"\n",
    "    For each JSON URL, read it once and build a lookup:\n",
    "        (json_url, row, col) -> [counts...]\n",
    "    Accepts row/col under 'row'/'col' or 'rownum'/'colnum'.\n",
    "    Accepts spectrum under 'aggregatedspectrum' / 'aggregatedSpectrum' / 'spectrum'.\n",
    "    \"\"\"\n",
    "    session = session or _new_session()\n",
    "    index: dict[tuple[str,int,int], list[int]] = {}\n",
    "\n",
    "    for url in tqdm(urls, desc=\"Downloading JSON files\", disable=not progress):\n",
    "        items = fetch_json_items(url, session)\n",
    "        for rec in items:\n",
    "            r = rec.get(\"rownum\", rec.get(\"row\"))\n",
    "            c = rec.get(\"colnum\", rec.get(\"col\"))\n",
    "            spec = rec.get(\"aggregatedspectrum\") or rec.get(\"aggregatedSpectrum\") or rec.get(\"spectrum\")\n",
    "            if r is None or c is None or spec is None:\n",
    "                continue\n",
    "            try:\n",
    "                key = (url, int(r), int(c))\n",
    "                index[key] = [int(x) for x in spec]\n",
    "            except Exception:\n",
    "                pass\n",
    "    return index\n",
    "\n",
    "\n",
    "def attach_spectra(df: pd.DataFrame,\n",
    "                   index: dict[tuple[str,int,int], list[int]],\n",
    "                   *, progress: bool = True) -> pd.DataFrame:\n",
    "    \"\"\"Add a 'spectrum' column to df using the (url,row,col)->spectrum index.\"\"\"\n",
    "    def pick(row):\n",
    "        return index.get((row[\"json_url\"], int(row[\"row\"]), int(row[\"col\"])), None)\n",
    "\n",
    "    df = df.copy()\n",
    "    if progress:\n",
    "        tqdm.pandas(desc=\"Indexing spectra\")\n",
    "        df[\"spectrum\"] = df.progress_apply(pick, axis=1)\n",
    "    else:\n",
    "        df[\"spectrum\"] = df.apply(pick, axis=1)\n",
    "    return df\n",
    "\n",
    "\n",
    "def stack_spectra(spec_series: pd.Series) -> tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Given a Series of list-like spectra, returns:\n",
    "        - stack: (n_spectra x max_len) array, zero-padded\n",
    "        - x: channel indices\n",
    "    \"\"\"\n",
    "    spec_series = spec_series.dropna()\n",
    "    spec_series = spec_series[spec_series.map(lambda x: isinstance(x, (list, tuple)) and len(x) > 0)]\n",
    "    if spec_series.empty:\n",
    "        raise ValueError(\"No valid spectra to stack.\")\n",
    "\n",
    "    max_len = int(max(len(s) for s in spec_series))\n",
    "\n",
    "    def pad_to(s, L):\n",
    "        a = np.asarray(s, dtype=np.int64)\n",
    "        if a.size < L:\n",
    "            a = np.pad(a, (0, L - a.size))\n",
    "        return a\n",
    "\n",
    "    stack = np.vstack([pad_to(s, max_len) for s in spec_series])\n",
    "    x = np.arange(max_len)\n",
    "    return stack, x\n",
    "\n",
    "\n",
    "def plot_cumulative(stack: np.ndarray, x: np.ndarray, *, title_prefix=\"Cumulative spectrum\"):\n",
    "    cum = stack.sum(axis=0)\n",
    "    n = stack.shape[0]\n",
    "    plt.figure(figsize=(10, 4.5))\n",
    "    plt.plot(x, cum)\n",
    "    plt.xlabel(\"Channel\")\n",
    "    plt.ylabel(\"Counts\")\n",
    "    plt.title(f\"{title_prefix} (n={n})\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    return cum\n",
    "\n",
    "\n",
    "def plot_overlay(stack: np.ndarray, x: np.ndarray, *, max_traces=50, title=\"Overlay spectra\"):\n",
    "    n = stack.shape[0]\n",
    "    plt.figure(figsize=(10, 4.5))\n",
    "    step = max(1, n // max_traces)\n",
    "    for i in range(0, n, step):\n",
    "        plt.plot(x, stack[i], alpha=0.25)\n",
    "    plt.xlabel(\"Channel\")\n",
    "    plt.ylabel(\"Counts\")\n",
    "    plt.title(f\"{title} (showing ~{min(n, max_traces)} of {n})\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Simple peak detection helpers\n",
    "# -----------------------------\n",
    "\n",
    "def baseline_als(y, lam=1e6, p=0.01, niter=10):\n",
    "    \"\"\"Asymmetric Least Squares baseline.\"\"\"\n",
    "    y = np.asarray(y, dtype=float)\n",
    "    L = y.size\n",
    "    D = sparse.diags([1, -2, 1], [0, 1, 2], shape=(L-2, L))\n",
    "    w = np.ones(L)\n",
    "    for _ in range(niter):\n",
    "        W = sparse.spdiags(w, 0, L, L)\n",
    "        Z = W + lam * (D.T @ D)\n",
    "        z = spsolve(Z, w * y)\n",
    "        w = p * (y > z) + (1 - p) * (y < z)\n",
    "    return z\n",
    "\n",
    "\n",
    "def preprocess(y, smooth_window=21, smooth_poly=3, do_baseline=True, lam=1e6, p=0.01):\n",
    "    \"\"\"Baseline-correct and smooth the signal.\"\"\"\n",
    "    y = np.asarray(y, dtype=float)\n",
    "    if smooth_window and smooth_window > 3:\n",
    "        w = min(int(smooth_window) | 1, len(y) - (1 - len(y) % 2))\n",
    "        w = max(5, w | 1)\n",
    "        y_s = savgol_filter(y, window_length=w, polyorder=min(smooth_poly, w - 1))\n",
    "    else:\n",
    "        y_s = y\n",
    "\n",
    "    if do_baseline:\n",
    "        b = baseline_als(y_s, lam=lam, p=p)\n",
    "        y_corr = np.clip(y_s - b, 0, None)\n",
    "    else:\n",
    "        y_corr = y_s\n",
    "\n",
    "    return y_corr\n",
    "\n",
    "\n",
    "def estimate_noise(y):\n",
    "    \"\"\"Robust noise estimate using MAD of first differences.\"\"\"\n",
    "    y = np.asarray(y, dtype=float)\n",
    "    if y.size < 3:\n",
    "        return 0.0\n",
    "    d = np.diff(y)\n",
    "    med = np.median(d)\n",
    "    mad = np.median(np.abs(d - med))\n",
    "    return 1.4826 * mad\n",
    "\n",
    "\n",
    "def detect_peaks(y, x=None, min_prom=None, min_height=None, min_distance=5,\n",
    "                 rel_height=0.5, max_peaks=None):\n",
    "    \"\"\"\n",
    "    Return (peaks_df, meta) where peaks_df has:\n",
    "        x, idx, height, prominence, fwhm, area\n",
    "    \"\"\"\n",
    "    y = np.asarray(y, dtype=float)\n",
    "    x = np.arange(len(y)) if x is None else np.asarray(x, dtype=float)\n",
    "\n",
    "    yc = preprocess(y)\n",
    "    noise = estimate_noise(yc)\n",
    "\n",
    "    if min_prom is None:\n",
    "        min_prom = 6 * noise\n",
    "    if min_height is None:\n",
    "        min_height = 3 * noise\n",
    "\n",
    "    idx, props = find_peaks(\n",
    "        yc,\n",
    "        prominence=min_prom,\n",
    "        height=min_height,\n",
    "        distance=min_distance\n",
    "    )\n",
    "\n",
    "    if idx.size == 0:\n",
    "        df = pd.DataFrame(columns=[\"x\", \"idx\", \"height\", \"prominence\", \"fwhm\", \"area\"])\n",
    "        return df, {\"noise\": noise, \"min_prom\": min_prom, \"min_height\": min_height}\n",
    "\n",
    "    results = peak_widths(yc, idx, rel_height=rel_height)\n",
    "    widths = results[0]\n",
    "    left_ips, right_ips = results[2], results[3]\n",
    "\n",
    "    # simple trapezoid area under the corrected curve\n",
    "    areas = []\n",
    "    for li, ri in zip(left_ips, right_ips):\n",
    "        li_i = max(0, int(np.floor(li)))\n",
    "        ri_i = min(len(yc) - 1, int(np.ceil(ri)))\n",
    "        areas.append(float(np.trapz(yc[li_i:ri_i+1], x[li_i:ri_i+1])))\n",
    "\n",
    "    fwhm = widths\n",
    "\n",
    "    df = pd.DataFrame({\n",
    "        \"x\": x[idx],\n",
    "        \"idx\": idx,\n",
    "        \"height\": props.get(\"peak_heights\", np.zeros_like(idx, dtype=float)),\n",
    "        \"prominence\": props.get(\"prominences\", np.zeros_like(idx, dtype=float)),\n",
    "        \"fwhm\": fwhm,\n",
    "        \"area\": areas,\n",
    "    }).sort_values(\"height\", ascending=False)\n",
    "\n",
    "    if max_peaks:\n",
    "        df = df.head(int(max_peaks)).sort_values(\"x\").reset_index(drop=True)\n",
    "    else:\n",
    "        df = df.sort_values(\"x\").reset_index(drop=True)\n",
    "\n",
    "    return df, {\"noise\": noise, \"min_prom\": min_prom, \"min_height\": min_height}\n",
    "\n",
    "\n",
    "def plot_with_peaks(y, x=None, peaks_df=None, title=\"Spectrum with detected peaks\"):\n",
    "    y = np.asarray(y, dtype=float)\n",
    "    x = np.arange(len(y)) if x is None else np.asarray(x, dtype=float)\n",
    "\n",
    "    plt.figure(figsize=(10, 4.5))\n",
    "    plt.plot(x, y, label=\"signal\")\n",
    "\n",
    "    if peaks_df is not None and not peaks_df.empty:\n",
    "        plt.scatter(peaks_df[\"x\"], peaks_df[\"height\"], marker=\"x\", label=\"peaks\")\n",
    "\n",
    "    plt.xlabel(\"Channel\")\n",
    "    plt.ylabel(\"Counts\")\n",
    "    plt.title(title)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# -----------------------------\n",
    "# Minimal X-ray line library (keV)\n",
    "# Extend this list as needed.\n",
    "# -----------------------------\n",
    "_COMMON_LINES_KEV = {\n",
    "    \"B\":  {\"Ka1\": 0.1833},\n",
    "    \"C\":  {\"Ka1\": 0.277},\n",
    "    \"N\":  {\"Ka1\": 0.3924},\n",
    "    \"O\":  {\"Ka1\": 0.5249},\n",
    "    \"F\":  {\"Ka1\": 0.6768},\n",
    "    \"Na\": {\"Ka1\": 1.0403},\n",
    "    \"Mg\": {\"Ka1\": 1.25379, \"Kb1\": 1.302},\n",
    "    \"Al\": {\"Ka1\": 1.4865,  \"Kb1\": 1.557},\n",
    "    \"Si\": {\"Ka1\": 1.7398,  \"Kb1\": 1.837},\n",
    "    \"P\":  {\"Ka1\": 2.0105,  \"Kb1\": 2.1395},\n",
    "    \"S\":  {\"Ka1\": 2.3095,  \"Kb1\": 2.465},\n",
    "    \"Cl\": {\"Ka1\": 2.622,   \"Kb1\": 2.812},\n",
    "    \"K\":  {\"Ka1\": 3.3138,  \"Kb1\": 3.5901},\n",
    "    \"Ca\": {\"Ka1\": 3.6923,  \"Kb1\": 4.0131},\n",
    "\n",
    "    \"Ti\": {\"Ka1\": 4.5122,  \"Kb1\": 4.9334, \"La1\": 0.4518, \"Lb1\": 0.4582},\n",
    "    \"Cr\": {\"Ka1\": 5.4149,  \"Kb1\": 5.9468, \"La1\": 0.5721, \"Lb1\": 0.5818},\n",
    "    \"Mn\": {\"Ka1\": 5.9003,  \"Kb1\": 6.4918, \"La1\": 0.6367, \"Lb1\": 0.6479},\n",
    "    \"Fe\": {\"Ka1\": 6.4052,  \"Kb1\": 7.0593, \"La1\": 0.7048, \"Lb1\": 0.7179},\n",
    "    \"Co\": {\"Ka1\": 6.9309,  \"Kb1\": 7.6491, \"La1\": 0.7751, \"Lb1\": 0.7902},\n",
    "    \"Ni\": {\"Ka1\": 7.4803,  \"Kb1\": 8.2668, \"La1\": 0.8487, \"Lb1\": 0.866},\n",
    "    \"Cu\": {\"Ka1\": 8.0463,  \"Kb1\": 8.9039, \"La1\": 0.9277, \"Lb1\": 0.9473},\n",
    "    \"Zn\": {\"Ka1\": 8.6372,  \"Kb1\": 9.5704, \"La1\": 1.0117, \"Lb1\": 1.0347},\n",
    "\n",
    "    \"Zr\": {\"Ka1\": 15.775,  \"Kb1\": 17.6682, \"La1\": 2.0442, \"Lb1\": 2.1259},\n",
    "    \"Nb\": {\"Ka1\": 16.615,  \"Kb1\": 18.6254, \"La1\": 2.1687, \"Lb1\": 2.26},\n",
    "    \"Mo\": {\"Ka1\": 17.48,   \"Kb1\": 19.606,  \"La1\": 2.2921, \"Lb1\": 2.3939},\n",
    "\n",
    "    \"Pd\": {\"La1\": 2.8378, \"Lb1\": 2.9895},\n",
    "    \"Ag\": {\"La1\": 2.9827, \"Lb1\": 3.15},\n",
    "    \"Sn\": {\"La1\": 3.4441, \"Lb1\": 3.6628},\n",
    "\n",
    "    \"Ta\": {\"La1\": 8.146,  \"Lb1\": 9.343},\n",
    "    \"W\":  {\"La1\": 8.398,  \"Lb1\": 9.672},\n",
    "    \"Pt\": {\"La1\": 9.442,  \"Lb1\": 11.071},\n",
    "    \"Au\": {\"La1\": 9.713,  \"Lb1\": 11.443},\n",
    "}\n",
    "\n",
    "\n",
    "def line_library(beam_keV=15.0, elements=None, include=(\"Ka1\",\"Kb1\",\"La1\",\"Lb1\")) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Build a line library DataFrame from _COMMON_LINES_KEV.\n",
    "    Returns both 'energy_keV' and 'E_keV' columns for compatibility.\n",
    "    \"\"\"\n",
    "    rows = []\n",
    "    use = elements if elements else _COMMON_LINES_KEV.keys()\n",
    "\n",
    "    for el in use:\n",
    "        d = _COMMON_LINES_KEV.get(el, {})\n",
    "        for line, E in d.items():\n",
    "            if include and line not in include:\n",
    "                continue\n",
    "            E = float(E)\n",
    "            if E <= beam_keV:\n",
    "                rows.append({\n",
    "                    \"element\": el,\n",
    "                    \"line\": line,\n",
    "                    \"energy_keV\": E,\n",
    "                    \"E_keV\": E,  # alias for convenience\n",
    "                    \"label\": f\"{el} {line}\"\n",
    "                })\n",
    "\n",
    "    df = pd.DataFrame(rows)\n",
    "    if df.empty:\n",
    "        return df\n",
    "\n",
    "    return df.sort_values(\"energy_keV\", ignore_index=True)\n",
    "\n",
    "\n",
    "def _fwhm_model_const(fwhm_eV=67.8):\n",
    "    f = float(fwhm_eV) / 1000.0  # keV\n",
    "    return lambda E_keV: f\n",
    "\n",
    "\n",
    "def _match_peaks_to_lines(peaks_df: pd.DataFrame,\n",
    "                          lines_df: pd.DataFrame,\n",
    "                          *,\n",
    "                          sigma: float = 2.5,\n",
    "                          fwhm_func=None) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Match each peak to nearest library line within sigma * σ(E).\n",
    "    Accepts either 'energy_keV' or 'E_keV' in lines_df.\n",
    "    \"\"\"\n",
    "    if peaks_df is None or peaks_df.empty or lines_df is None or lines_df.empty:\n",
    "        return pd.DataFrame(columns=[\n",
    "            \"energy_keV\",\"idx\",\"height\",\"prominence\",\"area\",\"fwhm\",\n",
    "            \"element\",\"line\",\"lib_energy_keV\",\"delta_keV\",\"label\"\n",
    "        ])\n",
    "\n",
    "    energy_col = \"energy_keV\" if \"energy_keV\" in lines_df.columns else \"E_keV\"\n",
    "    if energy_col not in lines_df.columns:\n",
    "        raise KeyError(\"lines_df must have 'energy_keV' or 'E_keV'.\")\n",
    "\n",
    "    if fwhm_func is None:\n",
    "        fwhm_func = _fwhm_model_const(67.8)\n",
    "\n",
    "    lib_E = lines_df[energy_col].to_numpy()\n",
    "    out = []\n",
    "\n",
    "    for _, p in peaks_df.iterrows():\n",
    "        e = float(p[\"x\"])\n",
    "        fwhm = float(fwhm_func(e))\n",
    "        sigma_E = fwhm / 2.355 if fwhm > 0 else 0.05\n",
    "\n",
    "        j = int(np.argmin(np.abs(lib_E - e)))\n",
    "        delta = float(lib_E[j] - e)\n",
    "\n",
    "        if abs(delta) <= sigma * sigma_E:\n",
    "            r = lines_df.iloc[j]\n",
    "            lib_e = float(r[energy_col])\n",
    "            out.append({\n",
    "                \"energy_keV\": e,\n",
    "                \"idx\": int(p.get(\"idx\", -1)),\n",
    "                \"height\": float(p.get(\"height\", np.nan)),\n",
    "                \"prominence\": float(p.get(\"prominence\", np.nan)),\n",
    "                \"area\": float(p.get(\"area\", np.nan)),\n",
    "                \"fwhm\": float(p.get(\"fwhm\", np.nan)),\n",
    "                \"element\": r.get(\"element\", \"\"),\n",
    "                \"line\": r.get(\"line\", \"\"),\n",
    "                \"lib_energy_keV\": lib_e,\n",
    "                \"delta_keV\": delta,\n",
    "                \"label\": r.get(\"label\", f\"{r.get('element','')} {r.get('line','')}\".strip()),\n",
    "            })\n",
    "\n",
    "    return pd.DataFrame(out).sort_values(\"energy_keV\", ignore_index=True)\n",
    "\n",
    "\n",
    "def identify_elements(cum,\n",
    "                      *,\n",
    "                      x_keV=None,\n",
    "                      eV_per_ch=20.000347,\n",
    "                      start_eV=-192.768,\n",
    "                      beam_keV=15.0,\n",
    "                      fwhm_Mn_eV=67.8,\n",
    "                      elements=None,\n",
    "                      include=(\"Ka1\",\"Kb1\",\"La1\",\"Lb1\"),\n",
    "                      max_peaks=30,\n",
    "                      sigma=2.5,\n",
    "                      min_distance=5,\n",
    "                      # Optional: let you pass a custom lines_df directly\n",
    "                      custom_lines_df=None):\n",
    "    \"\"\"\n",
    "    Identify likely element lines in an aggregate EDS spectrum.\n",
    "\n",
    "    Returns:\n",
    "        assign_df, peaks_df, lines_df, meta\n",
    "    \"\"\"\n",
    "    if \"detect_peaks\" not in globals():\n",
    "        raise RuntimeError(\"identify_elements needs detect_peaks defined earlier in the notebook.\")\n",
    "\n",
    "    cum = np.asarray(cum)\n",
    "\n",
    "    # Energy axis\n",
    "    if x_keV is None:\n",
    "        L = len(cum)\n",
    "        x_keV = (start_eV + np.arange(L) * eV_per_ch) / 1000.0\n",
    "    else:\n",
    "        x_keV = np.asarray(x_keV)\n",
    "\n",
    "    # Library\n",
    "    if custom_lines_df is not None:\n",
    "        lines_df = custom_lines_df.copy()\n",
    "        # normalise energy column + label\n",
    "        if \"energy_keV\" not in lines_df.columns and \"E_keV\" in lines_df.columns:\n",
    "            lines_df = lines_df.rename(columns={\"E_keV\": \"energy_keV\"})\n",
    "        if \"E_keV\" not in lines_df.columns and \"energy_keV\" in lines_df.columns:\n",
    "            lines_df[\"E_keV\"] = lines_df[\"energy_keV\"]\n",
    "        if \"label\" not in lines_df.columns:\n",
    "            lines_df[\"label\"] = lines_df[\"element\"].astype(str) + \" \" + lines_df[\"line\"].astype(str)\n",
    "        lines_df = lines_df[lines_df[\"energy_keV\"] <= beam_keV].sort_values(\"energy_keV\", ignore_index=True)\n",
    "    else:\n",
    "        lines_df = line_library(beam_keV=beam_keV, elements=elements, include=include)\n",
    "\n",
    "    # Peaks in keV space\n",
    "    peaks_df, meta = detect_peaks(\n",
    "        cum,\n",
    "        x=x_keV,\n",
    "        max_peaks=max_peaks,\n",
    "        min_distance=min_distance\n",
    "    )\n",
    "\n",
    "    # Match\n",
    "    fwhm_func = _fwhm_model_const(fwhm_Mn_eV)\n",
    "    assign_df = _match_peaks_to_lines(peaks_df, lines_df, sigma=sigma, fwhm_func=fwhm_func)\n",
    "\n",
    "    return assign_df, peaks_df, lines_df, meta\n",
    "\n",
    "\n",
    "def infer_dataset_base_from_api(api_url: str) -> str:\n",
    "    p = urlparse(api_url)\n",
    "    qs = parse_qs(p.query)\n",
    "    dataset = (qs.get(\"dataset\") or [None])[0]\n",
    "    if not dataset:\n",
    "        raise ValueError(\"Could not find 'dataset' parameter in the API URL.\")\n",
    "    root = f\"{p.scheme}://{p.netloc}/surface-viewer/data/\"\n",
    "    dataset_encoded = quote(dataset, safe=\"\")\n",
    "    return urljoin(root, dataset_encoded + \"/\")\n",
    "\n",
    "def load_config_txt(dataset_base: str) -> dict:\n",
    "    \"\"\"\n",
    "    Parse config.txt the same way the viewer does:\n",
    "    - strip comments after '#'\n",
    "    - split key=value\n",
    "    - lowercase keys\n",
    "    \"\"\"\n",
    "    url = urljoin(dataset_base, \"config.txt\")\n",
    "    r = requests.get(url, timeout=30)\n",
    "    if not r.ok:\n",
    "        return {}\n",
    "    txt = r.text\n",
    "    cfg = {}\n",
    "    for line in txt.splitlines():\n",
    "        s = re.sub(r\"#.*$\", \"\", line).strip()\n",
    "        if not s or \"=\" not in s:\n",
    "            continue\n",
    "        k, v = s.split(\"=\", 1)\n",
    "        cfg[k.strip().lower()] = v.strip()\n",
    "    return cfg\n",
    "\n",
    "def get_energy_cal_from_dataset(api_url: str,\n",
    "                                default_eV_per_ch=20.000347,\n",
    "                                default_start_eV=-192.768):\n",
    "    dataset_base = infer_dataset_base_from_api(api_url)\n",
    "    cfg = load_config_txt(dataset_base)\n",
    "\n",
    "    eV_per_ch = float(cfg.get(\"eds_ev_per_ch\", default_eV_per_ch))\n",
    "    start_eV  = float(cfg.get(\"eds_start_ev\", default_start_eV))\n",
    "\n",
    "    n_channels = cfg.get(\"eds_n_channels\", None)\n",
    "    n_channels = int(n_channels) if n_channels and str(n_channels).isdigit() else None\n",
    "\n",
    "    return {\n",
    "        \"dataset_base\": dataset_base,\n",
    "        \"eV_per_ch\": eV_per_ch,\n",
    "        \"start_eV\": start_eV,\n",
    "        \"n_channels\": n_channels,\n",
    "        \"raw_cfg\": cfg\n",
    "    }\n",
    "\n",
    "def make_energy_axis(cum, cal: dict):\n",
    "    n = len(cum) if cal.get(\"n_channels\") is None else min(len(cum), cal[\"n_channels\"])\n",
    "    x_keV = (cal[\"start_eV\"] + np.arange(n) * cal[\"eV_per_ch\"]) / 1000.0\n",
    "    return x_keV\n",
    "\n",
    "\n",
    "def plot_identified_elements_confident(\n",
    "    cum,\n",
    "    api_url,\n",
    "    assign_df,\n",
    "    peaks_df=None,\n",
    "    *,\n",
    "    top_n_labels=8,\n",
    "    beam_keV=15.0,\n",
    "    fwhm_Mn_eV=67.8,\n",
    "    sigma=2.5,\n",
    "    show_all_markers=True,\n",
    "    show_raw=True,\n",
    "    show_corrected=True,\n",
    "):\n",
    "    \"\"\"\n",
    "    Confidence-first plot for aggregate EDS identification.\n",
    "    - Uses dataset calibration from config.txt (viewer-compatible)\n",
    "    - Shows raw + baseline-corrected aggregate\n",
    "    - Marks detected peaks\n",
    "    - Labels top N matches by a confidence score\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    cum : array-like\n",
    "        Aggregate spectrum counts.\n",
    "    api_url : str\n",
    "        ROI API URL (used to infer dataset base + fetch config.txt).\n",
    "    assign_df : pd.DataFrame\n",
    "        Output from identify_elements (with lib_energy_keV, delta_keV, height/prominence where possible).\n",
    "    peaks_df : pd.DataFrame, optional\n",
    "        Output peaks table (from identify_elements call).\n",
    "    top_n_labels : int\n",
    "        How many labels to annotate.\n",
    "    fwhm_Mn_eV : float\n",
    "        Used for a constant resolution model in keV.\n",
    "    sigma : float\n",
    "        Matching tolerance used in the ID step; reused for scoring.\n",
    "    \"\"\"\n",
    "\n",
    "    # --- Calibration from dataset config ---\n",
    "    cal = get_energy_cal_from_dataset(api_url)\n",
    "    eV_per_ch = cal[\"eV_per_ch\"]\n",
    "    start_eV  = cal[\"start_eV\"]\n",
    "\n",
    "    n = len(cum)\n",
    "    if cal.get(\"n_channels\"):\n",
    "        n = min(n, cal[\"n_channels\"])\n",
    "\n",
    "    cum_plot = np.asarray(cum[:n], dtype=float)\n",
    "    x_keV = (start_eV + np.arange(n) * eV_per_ch) / 1000.0\n",
    "\n",
    "    # --- Preprocess for corrected display (same style as your peak pipeline) ---\n",
    "    cum_corr = preprocess(cum_plot)\n",
    "\n",
    "    # --- Guard ---\n",
    "    if assign_df is None:\n",
    "        assign_df = assign_df  # will fall through to empty branch\n",
    "\n",
    "    # --- Confidence scoring ---\n",
    "    # Convert FWHM -> sigma_E (keV)\n",
    "    fwhm_keV = fwhm_Mn_eV / 1000.0\n",
    "    sigma_E = fwhm_keV / 2.355 if fwhm_keV > 0 else 0.05\n",
    "\n",
    "    assign = assign_df.copy() if hasattr(assign_df, \"copy\") else None\n",
    "\n",
    "    if assign is not None and not assign.empty:\n",
    "        # Ensure numeric columns exist\n",
    "        if \"height\" not in assign.columns or assign[\"height\"].isna().all():\n",
    "            assign[\"height\"] = assign.get(\"area\", 0.0)\n",
    "\n",
    "        if \"prominence\" not in assign.columns or assign[\"prominence\"].isna().all():\n",
    "            assign[\"prominence\"] = assign[\"height\"]\n",
    "\n",
    "        if \"delta_keV\" not in assign.columns:\n",
    "            # fallback if some earlier version didn't include it\n",
    "            assign[\"delta_keV\"] = assign[\"lib_energy_keV\"] - assign[\"energy_keV\"]\n",
    "\n",
    "        # mismatch in units of detector sigma (smaller is better)\n",
    "        assign[\"z_mismatch\"] = (assign[\"delta_keV\"].abs() / max(sigma_E, 1e-6))\n",
    "\n",
    "        # Confidence score:\n",
    "        # strong prominence boosted, penalised by mismatch\n",
    "        assign[\"score\"] = assign[\"prominence\"] / (1.0 + assign[\"z_mismatch\"])\n",
    "\n",
    "        # Keep only lines in visible energy window\n",
    "        assign = assign[(assign[\"lib_energy_keV\"] >= x_keV[0]) & (assign[\"lib_energy_keV\"] <= x_keV[-1])]\n",
    "\n",
    "        # Pick top N by score\n",
    "        label_df = (\n",
    "            assign\n",
    "            .sort_values(\"score\", ascending=False)\n",
    "            .head(int(top_n_labels))\n",
    "            .sort_values(\"lib_energy_keV\")\n",
    "            .reset_index(drop=True)\n",
    "        )\n",
    "    else:\n",
    "        label_df = None\n",
    "\n",
    "    # --- Plot ---\n",
    "    plt.figure(figsize=(10, 4.8))\n",
    "\n",
    "    if show_raw:\n",
    "        plt.plot(x_keV, cum_plot, lw=1, alpha=0.35, label=\"aggregate (raw)\")\n",
    "\n",
    "    if show_corrected:\n",
    "        plt.plot(x_keV, cum_corr, lw=1.2, label=\"aggregate (baseline-corrected)\")\n",
    "\n",
    "    ymax = float(np.max(cum_corr if show_corrected else cum_plot)) if cum_plot.size else 1.0\n",
    "\n",
    "    # Mark detected peak positions if provided\n",
    "    if peaks_df is not None and not peaks_df.empty:\n",
    "        # heights in peaks_df are on corrected curve in your pipeline\n",
    "        try:\n",
    "            y_peaks = cum_corr[peaks_df[\"idx\"].to_numpy(dtype=int)]\n",
    "        except Exception:\n",
    "            y_peaks = None\n",
    "\n",
    "        if y_peaks is not None:\n",
    "            plt.scatter(peaks_df[\"x\"], y_peaks, marker=\"x\", zorder=5, label=\"detected peaks\")\n",
    "\n",
    "    # Draw all matched markers lightly\n",
    "    if show_all_markers and assign is not None and not assign.empty:\n",
    "        for _, r in assign.iterrows():\n",
    "            plt.axvline(float(r[\"lib_energy_keV\"]), ls=\"--\", lw=0.6, alpha=0.18)\n",
    "\n",
    "    # Emphasise + label top N confident matches\n",
    "    if label_df is not None and not label_df.empty:\n",
    "        # stagger text heights to reduce label collisions\n",
    "        levels = np.linspace(0.92, 0.65, num=len(label_df))\n",
    "\n",
    "        for i, (_, r) in enumerate(label_df.iterrows()):\n",
    "            e = float(r[\"lib_energy_keV\"])\n",
    "            lbl = str(r.get(\"label\", f\"{r.get('element','')} {r.get('line','')}\")).strip()\n",
    "\n",
    "            plt.axvline(e, ls=\"--\", lw=1.0, alpha=0.8)\n",
    "\n",
    "            plt.text(\n",
    "                e,\n",
    "                levels[i] * ymax,\n",
    "                lbl,\n",
    "                rotation=90,\n",
    "                va=\"top\",\n",
    "                ha=\"center\",\n",
    "                fontsize=9\n",
    "            )\n",
    "\n",
    "    plt.xlabel(\"Energy (keV)\")\n",
    "    plt.ylabel(\"Counts\")\n",
    "    plt.title(f\"Aggregate spectrum with top {top_n_labels} most confident labels\")\n",
    "    plt.legend(loc=\"upper right\", frameon=False)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Return useful objects for debugging/display\n",
    "    return {\n",
    "        \"cal\": cal,\n",
    "        \"x_keV\": x_keV,\n",
    "        \"cum_plot\": cum_plot,\n",
    "        \"cum_corr\": cum_corr,\n",
    "        \"assign_scored\": assign if assign is not None else None,\n",
    "        \"label_df\": label_df\n",
    "    }\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fba21ce6",
   "metadata": {
    "id": "fba21ce6"
   },
   "source": [
    "## 1. Load ROI selections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b28f150b",
   "metadata": {
    "id": "b28f150b"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Load ROI selections from the API\n",
    "df = load_roi_api(api_url)\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6faccc2c",
   "metadata": {
    "id": "6faccc2c"
   },
   "outputs": [],
   "source": [
    "\n",
    "if df.empty:\n",
    "    print(\"No selections returned from the API.\")\n",
    "else:\n",
    "    print(f\"Total selected areas (ROI pixels): {len(df)}\")\n",
    "    if \"basename\" in df.columns:\n",
    "        print(f\"Unique basenames (sites/maps): {df['basename'].nunique()}\")\n",
    "    if \"srcJson\" in df.columns:\n",
    "        print(f\"Unique spectrum JSON files referenced: {df['srcJson'].nunique()}\")\n",
    "\n",
    "    # Simple per-site breakdown (if basename exists)\n",
    "    if \"basename\" in df.columns:\n",
    "        display(df.groupby(\"basename\", dropna=False).size().sort_values(ascending=False).to_frame(\"n_selected\").head(20))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d6969eb",
   "metadata": {
    "id": "4d6969eb"
   },
   "source": [
    "## 2. Resolve spectrum URLs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51396c39",
   "metadata": {
    "id": "51396c39"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Add fully-qualified JSON URLs\n",
    "df = add_json_urls(df, api_url)\n",
    "df[[\"row\", \"col\", \"basename\", \"foldername\", \"srcJson\", \"json_url\"]].head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efe3a060",
   "metadata": {
    "id": "efe3a060"
   },
   "source": [
    "## 3. Download, attach, and aggregate spectra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "901c0168",
   "metadata": {
    "id": "901c0168"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Build spectrum index from the unique JSON files referenced in the ROI list\n",
    "urls = df[\"json_url\"].dropna().unique().tolist()\n",
    "print(f\"Will download {len(urls)} JSON file(s)\")\n",
    "\n",
    "index = build_spectrum_index(urls, progress=True)\n",
    "df = attach_spectra(df, index, progress=True)\n",
    "\n",
    "# How many selections successfully matched a spectrum?\n",
    "ok = df[\"spectrum\"].notna().sum()\n",
    "print(f\"Spectra attached for {ok} / {len(df)} selections\")\n",
    "\n",
    "# Stack + plot\n",
    "stack, x = stack_spectra(df[\"spectrum\"])\n",
    "cum = plot_cumulative(stack, x, title_prefix=\"Cumulative ROI spectrum\")\n",
    "plot_overlay(stack, x, title=\"Overlay of ROI spectra\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97570b3b",
   "metadata": {
    "id": "97570b3b"
   },
   "source": [
    "## 4. Simplest peak analysis on the aggregate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "T_iCmUgXKSew",
   "metadata": {
    "id": "T_iCmUgXKSew"
   },
   "outputs": [],
   "source": [
    "# Visualise baseline vs corrected for the aggregate\n",
    "cum_s = cum_plot  # or your smoothed version if you expose it\n",
    "baseline = baseline_als(cum_plot)\n",
    "\n",
    "plt.figure(figsize=(10, 4.5))\n",
    "plt.plot(x_keV, cum_plot, lw=1, alpha=0.4, label=\"raw aggregate\")\n",
    "plt.plot(x_keV, baseline, lw=1, label=\"estimated baseline\")\n",
    "plt.plot(x_keV, np.clip(cum_plot - baseline, 0, None), lw=1.2, label=\"baseline-corrected\")\n",
    "plt.xlabel(\"Energy (keV)\")\n",
    "plt.ylabel(\"Counts\")\n",
    "plt.title(\"Baseline correction helps peak finding\")\n",
    "plt.legend(frameon=False)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "300de1c6",
   "metadata": {
    "id": "300de1c6"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Simple peak detection on the cumulative spectrum\n",
    "peaks_df, meta = detect_peaks(cum, x=x, max_peaks=20)\n",
    "\n",
    "print(\"Noise estimate:\", meta[\"noise\"])\n",
    "print(\"min_prom:\", meta[\"min_prom\"])\n",
    "print(\"min_height:\", meta[\"min_height\"])\n",
    "\n",
    "peaks_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3c35dc9",
   "metadata": {
    "id": "d3c35dc9"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Plot with detected peaks marked (using the baseline-corrected heights)\n",
    "# We re-run preprocess so the marker heights are on the corrected curve.\n",
    "cum_corr = preprocess(cum)\n",
    "peaks_df_corr, _ = detect_peaks(cum, x=x, max_peaks=20)\n",
    "\n",
    "plt.figure(figsize=(10, 4.5))\n",
    "plt.plot(x, cum_corr, label=\"cumulative (corrected)\")\n",
    "if not peaks_df_corr.empty:\n",
    "    plt.scatter(peaks_df_corr[\"x\"], cum_corr[peaks_df_corr[\"idx\"]], marker=\"x\", label=\"peaks\")\n",
    "plt.xlabel(\"Channel\")\n",
    "plt.ylabel(\"Counts (baseline-corrected)\")\n",
    "plt.title(\"Aggregate spectrum – peak finding\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "RX03sW0lBwEW",
   "metadata": {
    "id": "RX03sW0lBwEW"
   },
   "outputs": [],
   "source": [
    "# 1) calibration + axis\n",
    "cal = get_energy_cal_from_dataset(api_url)\n",
    "x_keV = make_energy_axis(cum, cal)\n",
    "\n",
    "# 2) identify\n",
    "assign_df, peaks_df, lines_df, meta = identify_elements(\n",
    "    cum,\n",
    "    x_keV=x_keV,\n",
    "    beam_keV=15.0,\n",
    "    fwhm_Mn_eV=67.8,\n",
    "    max_peaks=25,\n",
    ")\n",
    "\n",
    "# 3) best confidence plot (easy knob)\n",
    "top_n_labels = 8\n",
    "\n",
    "_ = plot_identified_elements_confident(\n",
    "    cum,\n",
    "    api_url,\n",
    "    assign_df,\n",
    "    peaks_df=peaks_df,\n",
    "    top_n_labels=top_n_labels,\n",
    "    fwhm_Mn_eV=67.8,\n",
    "    show_all_markers=True,\n",
    ");\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "BJ08NVUOJl76",
   "metadata": {
    "id": "BJ08NVUOJl76"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
